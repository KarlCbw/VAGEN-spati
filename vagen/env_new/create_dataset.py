import os
from vagen.env_new import REGISTERED_ENV
import numpy as np
import yaml
from datasets import Dataset, load_dataset
from vagen.env_new.utils.env_utils import permanent_seed
def create_dataset_from_yaml(yaml_file_path: str, force_gen=False):
    """
    Create dataset from a YAML configuration file.
    
    Args:
        yaml_file_path (str): Path to the YAML configuration file
        force_gen (bool): Whether to force regeneration of existing datasets
        
    The YAML file should have the following structure:
    ```
    seed: 42
    train_path: path/to/train.parquet
    test_path: path/to/test.parquet
    env1:
        env_name: sokoban  # or frozenlake
        env_config:
            # parameters to override the default env config
        split: train  # or test
        size: 100  # number of instances
    env2:
        env_name: frozenlake
        env_config:
            # parameters to override the default env config
        split: test
        size: 50
    ```
    
    If the environment config class (e.g., SokobanConfig, FrozenLakeConfig) has a 
    generate_seeds(size) method, it will be used to generate seeds for that environment.
    """
    
    if isinstance(yaml_file_path, str):
        with open(yaml_file_path, 'r') as f:
            yaml_config = yaml.safe_load(f)
    else:
        yaml_config = yaml_file_path
    
    train_path = yaml_config.get('train_path')
    test_path = yaml_config.get('test_path')
    
    os.makedirs(os.path.dirname(train_path), exist_ok=True)
    os.makedirs(os.path.dirname(test_path), exist_ok=True)
    
    if not force_gen and os.path.exists(train_path) and os.path.exists(test_path):
        print(f"Dataset files already exist at {train_path} and {test_path}. Skipping generation.")
        print(f"Use --force-gen to override and regenerate the dataset.")
        return
    
    
    train_instances = []
    test_instances = []
    
    global_seed = yaml_config.get('seed', 42)
    permanent_seed(global_seed)
    
    
    for key, value in yaml_config.items():
        if key in ['train_path', 'test_path','seed']:
            continue
        
        env_name = value.get('env_name')
        custom_env_config = value.get('env_config', {})
        split = value.get('split', 'train')
        env_size = value.get('size', 100)
        
        env_config = REGISTERED_ENV[env_name]["config"](**custom_env_config)
        seeds_for_env = None
        if hasattr(env_config, 'generate_seeds'):
            seeds_for_env = env_config.generate_seeds(env_size)
            print(f"Using {len(seeds_for_env)} seeds generated by {env_name} config's generate_seeds method")
        else:
            seeds_for_env = np.random.randint(0, 2**31 - 1, size=env_size).tolist()
        for seed in seeds_for_env:
            env_settings = {
                'env_name': env_name,
                'env_config': custom_env_config,
                'seed': seed
            }
            
            instance = {
                "data_source": env_name,
                "prompt": [{"role": "user", "content": ''}],
                "extra_info": {"split": split, **env_settings}
            }
            
            if split == 'train':
                train_instances.append(instance)
            else:
                test_instances.append(instance)
    
    def make_map_fn(split):
        def process_fn(example, idx):
            return example
        return process_fn
        
    # Create datasets
    if train_instances:
        train_dataset = Dataset.from_list(train_instances)
        train_dataset = train_dataset.map(function=make_map_fn('train'), with_indices=True)
        train_dataset.to_parquet(train_path)
        print(f"Train dataset with {len(train_instances)} instances saved to {train_path}")
    
    if test_instances:
        test_dataset = Dataset.from_list(test_instances)
        test_dataset = test_dataset.map(function=make_map_fn('test'), with_indices=True)
        test_dataset.to_parquet(test_path)
        print(f"Test dataset with {len(test_instances)} instances saved to {test_path}")
    
    if not train_instances and not test_instances:
        print("No instances were generated. Check your YAML configuration.")
        
        
        
if __name__ == "__main__":
    yaml_file_path = {
        "seed": 42,
        "train_path": "./train_example.parquet",
        "test_path": "./test_example.parquet",
        "env1": {
            "env_name": "sokoban",
            "env_config": {
                "num_boxes": 3
            },
            "split": "train",
            "size": 2
        },
        "env2": {
            "env_name": "frozenlake",
            "env_config": {
                "is_slippery": False,
                "p":0.1
            },
            "split": "test",
            "size": 2
        }
    }
    create_dataset_from_yaml(yaml_file_path, force_gen=True)
    # load the dataset and print
    train_dataset = load_dataset('parquet', data_files={"train": "./train_example.parquet"}, split="train")
    test_dataset = load_dataset('parquet', data_files={"test": "./test_example.parquet"}, split="test")
    for i in range(2):
        print(train_dataset[i])
        print(test_dataset[i])