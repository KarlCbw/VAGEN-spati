{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verl.utils import hf_tokenizer, hf_processor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<image1asdasdqwa>', '<image2>', '<image3>']\n"
     ]
    }
   ],
   "source": [
    "text_template = \"The quick brown fox jumps over the lazy dog.<image1asdasdqwa>, <image2>, <image3>.\"\n",
    "import re\n",
    "image_keys=re.findall(r'<image[a-zA-Z0-9]*>', text_template)\n",
    "print(image_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "processor = hf_processor(model_name)\n",
    "tokenizer = hf_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mask(input_ids, attention_mask):\n",
    "    sptk_b = tokenizer.convert_tokens_to_ids('<|box_start|>')\n",
    "    sptk_e = tokenizer.convert_tokens_to_ids('<|box_end|>')\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(f\"DEBUG:input_ids.shape:{input_ids.shape}\")\n",
    "    batch_size = input_ids.shape[0]\n",
    "    seq_len = input_ids.shape[1]\n",
    "    \n",
    "    # Initialize output tensors with same shape as inputs\n",
    "    new_input_ids = input_ids.clone()\n",
    "    new_attention_mask = attention_mask.clone()\n",
    "    loss_mask = torch.zeros_like(input_ids)\n",
    "    new_loss_mask = torch.zeros_like(input_ids)\n",
    "    # Process each example in the batch\n",
    "    for b in range(batch_size):\n",
    "        # Count right padding tokens using attention mask\n",
    "        right_pad_tokens = (new_input_ids[b] == pad_token_id).sum().item()\n",
    "        \n",
    "        # Assert that initial padding tokens have attention mask of 0\n",
    "        assert torch.all(attention_mask[b, -right_pad_tokens:] == 0), \"right padding tokens must have attention mask of 0\"\n",
    "        \n",
    "        # Find special token indices\n",
    "        sptk_b_indices = (input_ids[b] == sptk_b).nonzero().flatten()\n",
    "        sptk_e_indices = (input_ids[b] == sptk_e).nonzero().flatten()\n",
    "        \n",
    "        # Create a mask for tokens that should compute loss\n",
    "        hole_pos=[] # initialize holes position list with last padding token position\n",
    "        for start_pos, end_pos in zip(sptk_b_indices, sptk_e_indices):\n",
    "            loss_mask[b][start_pos+1:end_pos] = 1\n",
    "            hole_pos.append(start_pos.item())\n",
    "            hole_pos.append(end_pos.item())\n",
    "        hole_pos.append(seq_len-right_pad_tokens)\n",
    "        assert new_input_ids[b][seq_len-right_pad_tokens]==pad_token_id\n",
    "        \n",
    "        # shift right to fill the wholes\n",
    "        holes_to_fill=1\n",
    "        for i in range(0,len(hole_pos)-1):\n",
    "            start_pos = hole_pos[i]\n",
    "            end_pos = hole_pos[i+1]\n",
    "            new_loss_mask[b,start_pos+1-holes_to_fill:end_pos-holes_to_fill]=loss_mask[b,start_pos+1:end_pos]\n",
    "            new_input_ids[b,start_pos+1-holes_to_fill:end_pos-holes_to_fill]=input_ids[b,start_pos+1:end_pos]\n",
    "            new_attention_mask[b,start_pos+1-holes_to_fill:end_pos-holes_to_fill]=attention_mask[b,start_pos+1:end_pos]\n",
    "            holes_to_fill+=1\n",
    "\n",
    "        valid_tokens = seq_len-right_pad_tokens-len(hole_pos)+1 # the number of non-special tokens and non-padding tokens\n",
    "        new_loss_mask[b][valid_tokens:]=0\n",
    "        new_input_ids[b][valid_tokens:]=pad_token_id\n",
    "        new_attention_mask[b][valid_tokens:]=0\n",
    "        \n",
    "    return new_input_ids, new_attention_mask, new_loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import verl.utils.torch_functional as verl_F\n",
    "import torch\n",
    "prompt_with_chat_template=''                    \n",
    "input_ids,attention_mask=verl_F.tokenize_and_postprocess_data(prompt=prompt_with_chat_template,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_length=1024,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        left_pad=False,\n",
    "                                        truncation=\"error\",\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(input_ids[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention_mask[:,:\u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/vagen/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3861\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3862\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3863\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3864\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3865\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/vagen/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    667\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 668\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[1;32m    670\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    671\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[0]))\n",
    "print(attention_mask[:,:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_mask[:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:input_ids.shape:torch.Size([1, 1024])\n",
      "I love you<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "new_input_ids, new_attention_mask, new_loss_mask=loss_mask(input_ids, attention_mask)\n",
    "print(tokenizer.decode(new_input_ids[0][:30]))\n",
    "print(new_attention_mask[0][:30])\n",
    "print(new_loss_mask[0][:30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
